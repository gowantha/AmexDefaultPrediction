The given dataset was too large, and was unable to be loaded on a Kaggle notebook. So I used the AMEX-Feather-Dataset which is a lightweight and very fast processing method instead of the default dataset. Multiple data samples with the same customer ID are only saved as the last entry. As the first step of the feature engineering process,, the samples with more than 75% of the values missing were removed, and the rest were imputed using median for numerical features and mode for categorical features. Then, categorical encoding and scaling was done. The class imbalance was also handled using SMOTE. The dataset was split 70%-30% and trained using KNeighborsClassifier, SVM Linear SVC and XGBoost models. SVM LinearSVC was used instead of SVM due to the high feature count. From these three model approaches, the XGBoost model yielded the best results. So I selected it as my final model, and did several hyperparameter tuning to improve the model accuracy. My best Kaggle score of 0.7826 was gained by an XGBoost model with the following hyperparameters: XGB with objective = 'reg:logistic', max_depth = 5, seed = 0, n_estimators =1000, eta = 0.05.